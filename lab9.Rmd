---
title: "Lab 9"
author: ""
date: ""
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#  Housing Data
For this lab, we will consider the housing data again. So far, we have discussed various models for predicting price, but now we will compare various models against each other. As a quick refresher, recall that there are 522 observations with the following variables:

* price: in 2002 dollars
* area: Square footage
* bed: number of bedrooms
* bath: number of bathrooms
* ac: central AC (yes/no)
* garage: number of garage spaces
* pool: yes/no
* year: year of construction
* quality: high/medium/low
* home style: coded 1 through 7
* lot size: sq ft 
* highway: near a highway (yes/no)


```{r}
fileName <- "https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/estate.csv"
housing_data <- read.csv(fileName)
head(housing_data)
#create new column for the age of house
```

\newpage
#  Cross Validation
##  Data Splitting
We will now compare a few different models using cross validation. Notice that each time we include a new variable, the RSS never increases.
```{r}
library(leaps)

p <- 12 #Choose the number of explanatory covariates in the MODEL MATRIX 
        #(i.e. for categorical variables, include all indicator variables)

out_leaps <- regsubsets(
  log(price)~., 
  data=housing_data[,-1], #remove the id column 
  nvmax=p
  )
reg.sum <- summary(out_leaps)

reg.sum$outmat
```
```{r}
data.frame(
  Adj.R2 = which.max(res.sum$adjr2),
  CP = which.min(res.sum$cp),
  BIC = which.min(res.sum$bic)
)
```

##  K-fold Cross Validation
As we discussed last week, one-time sample-splitting results highly depend on the random selection of the training and test set. K-fold Cross Validation is a procedure to alleviate such randomness. Besides, computationally K-fold Cross Validation is much more feasible than leave-one-out Cross Validation (LOOCV).

In the K-fold Cross Validation, here we set K=5. which means we split the data into 5 equal sized subsets. Then for each k=1,...,5, hold out the $k$th subset and train the model based on the other 4 subsets, calculate $k$th fold mean square error (MSE) as $MSE_k$. Finally obtain the total MSE by averaging $MSE_k, k=1,...,5$.

K-fold Cross Validation can be done manually, but can also be implemented using "glm" functions. 

```{r}
library("boot")
#fit model 2 with area and year
mod_cv_1 <- glm(log(price) ~ year + area, data = housing_data)
summary(mod_cv_1)
err_cv_1 <- cv.glm(housing_data, mod_cv_1, K=5)$delta[1]

#fit model 4 with area, quality, and year
mod_cv_2 <- glm(log(price) ~ year + area + quality, data = housing_data)
summary(mod_cv_2)
err_cv_2 <- cv.glm(housing_data, mod_cv_2, K=5)$delta[1]

#show cross validated errors
data.frame(
  model_1 = err_cv_1,
  model_2 = err_cv_2
)
```

#  Penalized Scores
Cross validation can be computationally expensive, since it requires refitting the model on many different "test sets." On Wednesday, we will discuss potential alternatives to cross validation which don't require sample splitting and only fit the model once. In particular, they assign a score to each model, but explicitly include a penalty for more complex models. In particular, the two scores we will discuss are AIC (Akaike information criterion) and BIC (Bayesian information criterion).

1. $R^2$ measures how well the fitted model predicts the data it was fitted on, and will always increase when we include additional covariates. 

2. Adjusted $R^2$ add adjustment to penalize for increasing model complexity, but it is still not good enough. 

3. AIC, BIC require model assumptions to be theoretically grounded, but work
well empirically even when the assumptions donâ€™t hold. 

Penalized Scores are calculated based on the whole dataset. Next we will use 3 models from the regsubsets results to compare the choice of the "best" one among three using different model selection criteria. Similar to golf, when R calculates AIC and BIC, a smaller (further left on a number line) score indicates a better model.

```{r}
#Create variables to store the criterion results from different models
r_squared <- rep(0,3) 
adj_r_squared <- rep(0,3)
aic <- rep(0,3)
bic <- rep(0,3)
cv_error <- rep(0,3)

#fit model 1 with year and area
model1 <- lm(log(price) ~ year + area, data = housing_data)
summary(model1)
sum1 <- summary(model1)
r_squared[1] <- sum1$r.squared
adj_r_squared[1] <- sum1$adj.r.squared
aic[1] <- AIC(model1)
bic[1] <- BIC(model1)

model1_cv <- glm(log(price) ~ year + area, data = housing_data)
cv_error[1] <- cv.glm(housing_data, model1_cv, K=5)$delta[1]

#fit model 4 with year, area and quality
model2 <- lm(log(price) ~ year + area + quality, data = housing_data)
summary(model2)
sum2 <- summary(model2)
r_squared[2] <- sum2$r.squared
adj_r_squared[2] <- sum2$adj.r.squared
aic[2] <- AIC(model2)
bic[2] <- BIC(model2)

model2_cv <- glm(log(price) ~ year + area + quality, data = housing_data)
cv_error[2] <- cv.glm(housing_data, model2_cv, K=5)$delta[1]

#fit model 3 with year, area, quality, and lot
model3 <- lm(log(price) ~ year + area + quality + lot, data = housing_data)
summary(model3)
sum3 <- summary(model3)
r_squared[3] <- sum3$r.squared
adj_r_squared[3] <- sum3$adj.r.squared
aic[3] <- AIC(model3)
bic[3] <- BIC(model3)

model3_cv <- glm(log(price) ~ year + area + quality + lot, data = housing_data)
cv_error[3] <- cv.glm(housing_data, model3_cv, K=5)$delta[1]

#compare three models
name = c('model1', 'model2', 'model3')
rbind(name, r_squared, adj_r_squared, aic, bic, cv_error)
```

#### Questions

1. Compare the different criterions above, which model do you think is the best?

2. Suppose you have chosen the best one among the three, is that possible we could find a better one other than these three models? 



